---
layout: post
title: "하둡이 뭔데?"
comments: true
description: ""
keywords: ""
---


즐겁게 잘 지내던 팀에서 더 큰 그림을 그리기 위해 새로운 팀으로 자리를 옮겼다.


새로운 팀에 합류하여 기본적인 인프라 구성과 서비스 아키텍처에 대해서 설명을 들었다.


설명을 듣고 이해하는척 했지만 하나도 이해가 안됐다.

~~열심히 설명해주신 선배님들 정말 죄송합니다... 앞으로 자주 되묻겠습니다...~~ 

그동안 정말 나는 우물안 개구리였다. 

이제라도 알았으니 새로운 우물로 갈아타기 위해 삽을 들어야겠다.



참고로 이 글은 참고용이며 틀린부분이 있을 수 있으며, 살짝 맛만 볼 수 있는 글이니 경계하면서 읽자.

---



서비스 전체적인 리뷰시간에 들었던 내용중에 핵심 키워드(라고 쓰고 내가 모르는 것들)을 정리해보자면

결국 **하둡, 배치, 자바** 로 나눌 수 있겠다.



그중 내가 아무것도 몰라서 뭘 모르는지 질문조차 할수 없던것이 하둡이었다.

~~자바, 배치는 어떤 개념인진 아니까~~



설명을 듣는 1시간동안 내게 쏟아진 키워드 들을 더 세분화 해보자면

- Hadoop
- HDFS
- OwFS
- Hive
- Hbase
- Spark
- Presto
- Yanagishima
- C3
- Cuve
- Zeppelin
- Hue
- Kafka
- Storm



진짜 이름만 들어도 다 고만고만한놈들이 사람 헷갈리게 만든다.

~~우선 사내 인프라인 C3, Cuve, OwFS는 다음에 알아 보도록 하겠다. 왜냐면 다른게 더 중요하니까!~~



구글링을 해보니 결국 하둡안에 있는 거대한 에코시스템 덩어리들중 하나씩 자리잡고 있는 친구들이었다.



![ecosystem-1]('../images/hadoop/ecosystem-1.png')



"하둡 에코시스템"을 검색하면 흔히 볼 수 있는 이미지다.



뭔가 추상화해서 정리했는데 나같은 입문자에겐 이런 추상화된 이미지는 오히려 이해가 잘 되지 않는것 같다.

그래서 더 큰 이미지를 찾아봤다.



![ecosystem-2]('../images/hadoop/ecosystem-2.png')



뭔가 많다 근데 내가 들어본 키워드들이 다 들어가있다. 이제 대충 위에 키워드들이 어디서 어떤 역할들을 하는지 살짝 느낌이 오기 시작한다.



그럼 이제 하둡이 뭔지 알아보자



## Hadoop

하둡이 뭐고 왜 정말 진짜 이상한이름 Oozie, zookeeper 을 쓰는지 모르겠다.

오지? 주키퍼? 저기 강원도 산골 오지 이런건가? 동물원문지기? 작명이 왜이런지 모르겠다.



뭐 어쨋든 하둡은 데이터의 구조에 상관없이, 저렴한 비용으로 큰 데이터를 저장하는게 핵심이다.

크다는 의미는 100기가 바이트 그 이상을 말한다. 

기존의 RDBMS는 구조적인 요구사항이 처리할 수 있는 데이터의 종류를 제한하기 때문에 구조화된 데이터를 처리하는데 능숙하다.

반면에 이런 특징은 **데이터 웨어하우스**가 엄청나게 많은 데이터를 **빠르게** 탐색하기 어렵게 한다.

> 데이터 웨어하우스란 사용자의 의사결정에 도움을 주기 위해 다양한 시스템에서 데이터를 추출, 변환, 요약하여 능동적으로 사용자한테 제공할 수 있는 데이터베이스의 집합체를 말한다.
>
> 기본적으로 RDB가 있는 상태를 가정하여 데이터 웨어하우스를 구성한다. 데이터 웨어하우스는 데이터베이스와는 차원이 다른 데이터를 저장하고, 특히 무형식 데이터가 많으므로 데이터 웨어하우스 프로그램에서 유용한 정보를 뽑아내는 과정이 필요하다. 예를 들어 데이터베이스에는 동영상이나 음악파일을 저장할 수 없는데 데이터 웨어하우스에는 동영상/음악 파일 중 필요한 부분을 추출하여 보여줄 수 있다.



하지만 하둡을 사용한다면 ㅇㅋ!

Hadoop 프로젝트가 성숙하면서, Hadoop은 Hadoop의 사용성과 기능성을 위한 많은 컴포넌트들을 수용하였고, “Hadoop”이라는 단어는 Hadoop, 그리고 Hadoop과 관련된 다양한 컴포넌트들을 아우르는 전반적인 생태계를 지칭하게 되었다. 이것은 Linux의 경우와 비슷합니다. 엄밀히 말하자면 Linux는 Linux Kernel을 지칭하는 것이지만, 결국에 Linux를 하나의 완전한 운영체제로 부르고 있는 것과 같다.





#### 하둡의 핵심 1. MapReduce

MapReduce는 하둡에만 있는것이 아니다. 몽고디비에도 존재하고있다. (또 다른 여러 곳에도 존재)

MapReduce의 가장 중요한 혁신은 데이터셋을 나누고, 다수의 노드들에 대해 병렬적으로 실행하는 방식으로 통해 질의할 수 있는 능력이다. 연산을 나누는 것은 하나의 머신에 대하여 지나치게 큰 데이터를 처리하는 것에 대한 문제를 해결해 준다.

#### 하둡의 핵심 2. HDFS

MapReduce부터 다수의 서버들을 통해 분산 컴퓨팅까지 논했다. 컴퓨팅이 일어나기 위해서는 각 서버들은 데이터를 가지고 있어야 한다. 이것이 바로 HDFS(Hadoop Distributed File System)의 역할인데, 쉽게 말하면 HDFS는 데이터가 복제되고 클러스터를 통해 분산될 수 있도록 해준다. 연산이 끝나게 되면, 노드는 연산의 결과를 HDFS에 기록하는 역할을 한다.

HDFS 저장소에는 데이터에 대한 제약이 없다. 데이터는 구조화 되지 않을 수 있고, 일정한 스키마가 존재하지 않을 수 있다. 때문에 유연하게 다양한 데이터를 받아들일 수 있다. 반면에 RDBMS는 구조화 되어있기 때문에 유연하지 않다.

HDFS를 사용하게 된다면 데이터에 대한 책임은 개발자의 코드가 갖게 된다.

Mapreduce 레벨에서 Hadoop을 프로그래밍하는 것은 Java API를 통해 작업을 하는 것이라 할 수 있고, HDFS 안으로 데이터를 수동적으로 데이터 파일을 HDFS로 로드해 오는 것이라 할 수 있다.



#### 개발자의 생산성 향상. Pig와 Hive

Java API를 통해 직접 작업하는것은 에러가 발생하기 쉽다. 또한 이것은 Java 개발자들에게 하둡의 사용법을 제한할 수 있다.

때문에 하둡은 하둡 프로그래밍을 더 쉽게 도와주는 Pig와 Hive 라는 솔루션을 제공한다.

 Pig는 Hadoop의 공통 Task들(데이터를 로드해오거나, 데이터 변환을 표현하는 방법, 마지막 결과를 저장하는 방법 등)을 단순화 시켜놓은 프로그래밍 언어다. Pig의 내장 연산들은 마치 로그 파일 같은 반 구조화된 데이터라 할 수 있다. 그리고 Pig는 Java를 통해 확장이 가능하고, 커스텀 데이터 타입이나 변환 등을 제공할 수 있다.
Hive는 Hadoop을 데이터 웨어하우스처럼 사용할 수 있게 해준다. Hive는 HDFS에 있는 데이터를 구조화 시키고, SQL과 유사한 문법으로 데이터를 질의할 수 있도록 해준다. Pig처럼 Hive의 핵심 요소들은 확장 가능하다.

Hive와 Pig 중에 무엇을 선택해야할지 혼란스러울 것이다. Hive는 데이터 웨어하우징 작업, 정적 데이터, 빈번한 분석이 요구되는 경우에 적합하다. Hive의 SQL에 가까운 문법은 Hadoop과 다른 비즈니스 도구들과의 통합에 이상이다.

#### 데이터 접근의 향상. Hbase, Sqoop and Flume

태생적으로, Hadoop은 Batch 기반 시스템이다. 데이터들은 HDFS로 로드되어지고, 처리되며, 검색된다. 이는 다소 구식의 방식이고, 종종 대화식으로 작업되거나 랜덤 엑세스가 되어야 하기도 한다.





















---

출처 및 인용

- https://www.researchgate.net/figure/Apache-Hadoop-Ecosystem-architecture_fig1_334126337
- https://mydataexperiments.files.wordpress.com/2017/04/hadoop-ecosystem.png
- [https://mksung.wordpress.com/it-budget-2/apache-hadoop%EC%9D%98-%EA%B0%84%EB%8B%A8-%EC%A0%95%EB%A6%AC/](https://mksung.wordpress.com/it-budget-2/apache-hadoop의-간단-정리/)
- https://blrunner.tistory.com/99